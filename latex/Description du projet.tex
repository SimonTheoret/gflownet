\documentclass[11pt]{article}
% This file will be kept up-to-date at the following GitHub
%  repository: https://github.com/alexhernandezgarcia/latex-simple
%  Please file any issues/bug reports, etc. you may have at:
%  https://github.com/alexhernandezgarcia/latex-simple/issues

\usepackage{microtype} % microtypography
\usepackage{booktabs} % tables
\usepackage{url} % urls

% AMS math
\usepackage{amsmath} \usepackage{amsthm} \usepackage{amssymb}

% With no package options, the submission will be anonymized, the
%  supplemental material will be suppressed, and line numbers will be
%  added to the manuscript.  To hide the supplementary material (e.g.,
%  for the first submission deadline), use the [hidesupplement]
%  option: \usepackage[hidesupplement]{latex-simple} To compile a
%  non-anonymized camera-ready version, add the [final] option (for
%  the main track) e.g., \usepackage[final]{latex-simple} or
%  \usepackage[final, hidesupplement]{latex-simple}

\usepackage[final]{latex-simple}

% You may use any reference style as long as you are consistent
%  throughout the document. As a default we suggest author--year
%  citations; for bibtex and natbib you may use:

\usepackage{natbib}

% and for biber and biblatex you may use:

% \usepackage[% backend=biber, style=authoryear-comp, sortcites=true, natbib=true, giveninits=true, maxcitenames=2, doi=false, url=true, isbn=false, dashed=false ]{biblatex} \addbibresource{...}

\usepackage{chessboard} \usepackage{skak} \usepackage{float}
\usepackage{tikz}
\ExplSyntaxOn %requires texlive 2020, in older system load expl3
\cs_new:Npn \getfieldnumber #1 { \fp_eval:n { (\tl_tail:V #1 -1)*8 +
    \exp_args:Ne\int_from_alph:n{\tl_head:V #1} -1} } \ExplSyntaxOff

\title{GFlowChess: Joueur d'échec basé sur GFlowNet et sur Stockfish}

% The syntax for adding an author is \author[i]{\nameemail{author
%  name}{author email}} where i is an affiliation counter. Authors may
%  have multiple affiliations; e.g.:
%  \author[1,2]{\nameemail{Anonymous}{anonymous@example.com}}
\author[1]{\nameemail{Yizhan Li}{yizhan.li@umontreal.ca}}
\author[1]{\nameemail{Olivier
    Déry-Prévost}{olivier.dery-prevost@umontreal.ca}}
\author[1]{\nameemail{Sidya Galakho}{sidya.galakho@umontreal.ca}}
\author[1]{\nameemail{Simon Théorêt}{simon.theoret.1@umontreal.ca}}

% the list might continue: \author[2,3]{\nameemail{Author
%  2}{email2@example.com}} \author[3]{\nameemail{Author
%  3}{email3@example.com}} \author[4]{\nameemail{Author
%  4}{email4@example.com}}

% if you need to force a linebreak in the author list, prepend an
%  \author entry with \\:

% \author[3]{\\\nameemail{Author 5}{email5@example.com}}

% Specify corresponding affiliations after authors, referring to
%  counter used in \author:

\affil[1]{Université de Montréal}

% the list might continue: \affil[2]{Institution 2}
%  \affil[3]{Institution 3} \affil[4]{Institution 4}

% define PDF metadata, please fill in to aid in accessibility of the
%  resulting PDF
\hypersetup{%
  pdfauthor={}, % will be reset to "Anonymous" unless the "final" package option is given
  pdftitle={}, pdfsubject={}, pdfkeywords={} }

\begin{document}

\maketitle

% CCC: Contexte Contenu Conclusion

\begin{abstract}
  Les joueurs artificiels d'échec les plus performant tel que
  AlphaZero, basés sur les méthodes d'apprentissage par renforcement,
  sont en mesure de battre les meilleurs joueurs d'échecs humain.
  Dans ce rapport, nous présentons GFlowChess, un modèle génératif
  basé sur la famille de modèles GFlownet capable de simuler une
  partie d'échec suivant les règles traditionnelle du jeu à l'aide du
  jeu autonome (\textit{self-play}). Nous évaluons la performance du
  modèle à l'aide de Stockfish, un moteur d'échec moderne capable
  d'évaluer la probabilité de victoire d'une partie. Nous avons testé
  deux types de pertes ainsi que deux fonctions de récompenses pour
  observer le déroulement des trajectoires. Nous avons en effet été en
  mesure de constater les effets des fonctions de récompenses sur la
  distribution des trajectoires échantillonnées et de favoriser par
  l'entremise de la fonction de récompense un joueur. Nous discutons
  aussi les limitations associées au jeu autonome rencontrées durant
  l'entraînement.
\end{abstract}

\section*{Introduction}
% \subsection*{Les échecs et l'apprentissage automatique}
% TODO: À retirer?
Les échecs sont un jeu intellectuel notoirement difficile, compte tenu
du nombre important de règles, de pièces et variations possibles d'un
échiquier. En effet, le nombre de Shannon, qui représente une borne
inférieure sur le nombre de parties d'échec légales possibles, est
estimé à $10^{120}$. Malgré cette très grande complexité, d'important
moteur d'échec on été introduit au cours des deux dernières
décennies. Des moteurs d'échec, tel que Stockfish et AlphaZero, tel
que décrit dans \citet{AlphaZero}, on dramatiquement bouleversé la
scène professionnelle, en introduisant des joueurs artificiels
entraînés à l'aide de l'apprentissage par renforcement. Ces mêmes
modèles sont aujourd'hui les joueurs les plus performants, atteignant
des pointage \textit{elo} surhumain.

L'immense taille de l'espace de matchs possibles et l'aspect discret
et séquentiel des échecs font de la famille GFlowNet un bon candidat
pour échantillonner des parties d'échec et découvrir des stratégies
innovantes. Cependant, les modèles tels que Stockfish et AlphaZero
étant déjà extrêmement performants et capable de générer des
stratégies peu communes, nous nous sommes attardés à comprendre
comment nous pouvons influencer les comportements du modèles durant
une partie en modifiant la fonction de perte ainsi que la fonction de
récompense.

Nous avons entraîné notre modèle à l'aide de deux fonctions de pertes:
la fonction de perte \textit{Trajectory Balance Loss} introduit par
\citet{TBLoss} ainsi que \textit{Forward Looking Loss}, décrit dans
\citet{FLLoss}. Cette dernière fonction de perte à été choisie dans le
but de donner des récompenses intermédiaires au modèles durant la
construction des trajectoires. Intuitivement, cette approche
permettrait de guider plus efficacement le modèle vers une stratégie
efficace. De plus, nous avons entraîné GFlowChess avec types de
fonctions de récompenses: une fonction récompensant le modèle lorsque
les blancs sont avantagés et une autre récompensant des parties
égales. Finalement, notre fonction de récompense est dictée par le
moteur d'échec Stockfish, qui évalue l'échiquier et renvoie les
statistiques associées.

Notre entraînement avec la \textit{Forward Looking Loss} c'est avéré
infructueux: la fonction de perte n'a jamais convergé vers une perte
acceptable. Nous sommes incertain si la cause provient d'une erreur
d'implémentation ou bien des récompenses intermédiaires qui
ralentissent la convergence. Nous avons cependant été en mesure
d'entraîner avec succès GFlowChess avec la \textit{Forward Looking
  Loss} ainsi que nos deux fonctions de proxy. Nous avons observé
d'importantes variations du comportement dû au choix du proxy,
impliquant que GFlowChess est capable d'apprendre à échantillonner des
coups avantageux en fonction du proxy choisi.

\section*{Revue de la littérature}
Les GFLowNets sont une méthode d’échantillonage capable de
sélectionner des candidats diversifiés depuis une fonction de
récompense. Les méthodes GFlowNets étaient initialement présentées
comme un substitut au méthodes d’inférences amorties, telle que la
chaîne de Markov Monte Carlo (MCMC), \cite{gflownetfoundation}. Les
GFlowNet sont une famille de modèles génératifs récents, rendant la
littérature limitée. Néanmoins, l'intersection de l'apprentissage
automatique et des échecs est un domaine de recherche et
d'expérimentation actif depuis plusieurs années. Parmi le
développements les plus récents, nous pouvons compter Stockfish, le
moteur d'échec dont l'architecture est décrite dans par
\citet{Stockfish}. AlphaZero, adapté aussi bien échecs qu'au jeu de
Go, est un développement récent démontrant les capacités surhumaines
des modèles de d'apprentissage automatique. Finalement, le projet Maia
Chess, développé par \citet{Maia}, est application nouvelle de
l'apprentissage machine qui entraîne un modèle à jouer comme un humain
à l'aide de l'apprentissage par renforcement basé sur les commentaires
humains (RLHF).

Les certaines avancées récentes en lien avec les GFlowNets ont été
utilisés dans le rapport. Notamment l'usage de la fonction de perte
\textit{Trajectory Balance Loss}, qui se présente comme supérieure aux
fonctions précédentes dans le cas de longue trajectoire. La
\textit{Forward Looking Loss}, qui permet de calculer la récompense
aux étapes intermédiaires au lieu de calculer la récompense uniquement
à la fin de la trajectoire, a été utilisé afin de favoriser les coups
de qualité.

Quoique notre modèle est entraîné en jeu autonome dans un
environnement déterministe, des progrès ont été fait pour permettre
d'entraîner GFlowNet dans des environnements
stochastiques. \citet{stochasticflow} on notamment entraîné un modèle
GFlowNet dans un environnement stochastique, pavant la voix pour
l'entraînement d'un modèle GFlowNet contre un programme d'échec.

\section*{Résultats}

\subsection*{Librairies et ressources}
Nous avons fait principalement usage de 2 librairies:
\citet{hernandez-garcia2024} et \textit{Python Chess}. La première
librairie permet de faire abstraction du modèle GFlowNet et de ne
concevoir que l'environnement. La seconde librairie nous permet de
gérer la complexité des échecs et fournit presque tous les éléments
nécessaire à l'implémentation d'un environnement complet d'échec. De
plus, pour être en mesure d'évaluer la qualité des parties ainsi que
des coups jouer par GFlowChess, nous avons fais usage du moteur
d'échec Stockfish. Ce dernier est une variété de modèle de réseau de
neurone optimisé pour l'évaluation de jeux tels que le \textit{shogi}
et les échecs. Pour augmenter la vitesse d'entraînement, nous avons
paralléliser à l'aide du module Python \textit{multiprocessing} la
fonction de proxy.

\subsection*{Environnement et simplifications}
Nous avons modélisé l'environnement avec l'aide de l'objet
\textit{Board} de la librairie \textit{Python Chess}. Cet objet
contient tous les coups passé, le nombre de coups joués jusqu'à
présent, la position des pièces sur l'échiquier et les coups possibles
d'un côté et de l'autre. Nous avons modéliser les actions comme étant
un pair d'entier appartenant à $[0,63]^{2}$. Cela correspond à
associer à tout mouvement d'une pièce une case initiale,
potentiellement vide, ainsi qu'une case finale, pouvant être vide ou
pleine.  Cette modélisation simple nous permet d'interfacer facilement
avec l'objet \textit{Board} puisque cette notation est similaire à la
notation UCI. De plus, elle permet de représenter un échiquier sans
ambiguïté comme suit:

\begin{figure}[H]
  \centering \setchessboard{color=black,clearboard,showmover=false}
  \chessboard[ pgfstyle= {[base,at={\pgfpoint{0pt}{-0.3ex}}]text},
    text= \fontsize{1.2ex}{1.2ex}\bfseries
    \sffamily\getfieldnumber\currentwq, markboard]
  \caption{cases du tableau d'échiquier encodées en chiffres}
	\label{figcases}
\end{figure}

Cette façon de modéliser les actions possibles de l'agent nous permet
de facilement masquer les actions invalides:

\begin{figure}[H]
  \centering
	\begin{subfigure}[b]{0.45\textwidth}
	  \centering \setchessboard{showmover=false}
          \chessboard[setfen=r5k1/1b1p1ppp/p7/1p1Q4/2p1r3/PP4Pq/BBP2b1P/R4R1K
            w - - 0 20, pgfstyle=border,markfields={d4,d6},
            color=blue!50, colorbackfield=c5, pgfstyle=color,
            opacity=0.5, color=red, markfield={d5}]
	  \caption{Mouvements valides}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	  \centering \setchessboard{showmover=false}
          \chessboard[setfen=r5k1/1b1p1ppp/p7/1p6/2p1r3/PP1Q2Pq/BBP2b1P/R4R1K
            b - - 0 20, pgfstyle=border,markfields={d4,d6},
            color=blue!50, colorbackfield=c5, pgfstyle=color,
            opacity=0.5, color=red, markfield={d5}]
	  \caption{Mouvement invalide}
	\end{subfigure}
	\caption{ En adoptant l'encodage des cases de la figure 2, la
          reine Blanche à la case 35 peut effectuer les actions
          légales: [34,33], [34,42] et [34,26] comme montré dans la
          figure \ref{figcases}.  Donc l'action [34,18] est invalide
          donc il serait masqué.}
\end{figure}
Nous avons implémenter une fonction (\textit{getParents})nous
permettant d'obtenir, pour un état donné, tous les états qui auraient
pu mener à ce même état. Or, pour mettre en oeuvre cette fonction,
nous avons fait une simplification concernant les roques. Les roques
sont un mouvements permettant d'échanger sur la même ligne le roi et
un tour de la même couleur. En effet, ceux-ci sont complexes à
modélisés et sont rares en pratique. De plus, nous avons limité à 20
coups la durée des parties, les rendant un plus courte que la partie
d'échec moyenne. Cette fonction \textit{getParents} est complexe à
implémenter de par l'asymétrie des mouvements des pions.

Finalement, nous avons implémenter un \textit{FenParser}, c'est-à-dire
une transformation prenant un \textit{Board} et le transformant en un
tenseur de $N\times64\times13$, où $N$ est la taille du lot
(\textit{Batch}), 64 corresponds au 64 cases de l'échiquier et $13$
correspond au 13 pièces possibles sur une case.

\subsection*{Fonctions de perte et de récompense}
Nous avons utilisé deux fonctions de pertes pour entraîner nos
modèles: la fonction \textit{Trajectory Balance} ainsi que la fonction
\textit{Forward Looking}. De plus, nous avons fait usage de deux
fonctions de récompenses suivantes:
\begin{equation*}
  R_{1}(x) = - \mathbb{E}[X], \quad \text{où X=1 si les blancs gagne, 0 sinon},
\end{equation*} ainsi que
\begin{equation*}
  R_{2}(x) = - \frac{1}{1+10^{|c|/4}}, \quad \text{où $c$ est le score \textit{centipawn}}.
\end{equation*}Le score centipawn est une mesure de l'avantage des
blancs par rapport aux noirs. Nous avons normaliser le score centipawn
original de façon à ce qu'un score centipawn de -1 est équivalent à ce
que le joueur blanc perde un pion. Inversement, un score centipawn de
1 est équivalent à ce que le joueur noir perde un pion. La fonction de
récompense $R_{1}$ devrait inciter GFlowChess à faire gagner les
blancs, alors que la fonction $R_{2}$ devrait inciter GFlowNet à jouer
un match aussi serré que possible, sans avantagé aucun joueur.

\section*{Résultats et échecs}
\subsection*{Entraînements et échecs}
Nous avons tenté d'entraîner GFlowChess à l'aide de la fonction de
perte \textit{Forward Looking}. Or, la fonction de perte ne converge
pas sur une période de 5000 itérations.  Nous sommes incertain si la
cause provient d'une erreur d'implémentation de notre part ou bien des
récompenses intermédiaires qui ralentissent la convergence.
\begin{figure}[H]
  \centering \includegraphics[scale=0.15]{fwloss.png}
  \caption{Perte Forward Looking du modèle sur 5000 itérations}
	\label{fig:fwloss}
\end{figure}

Nous avons été en mesure d'entraîner pour 5000 itérations un modèle
GFlowNet utilisant la perte \textit{Trajectory Balance} ainsi que
l'une des deux fonction de récompense $R_{1}$ ou $R_{2}$ . De plus,
nous avons limité à 20 coups la durée des séquences.
\subsection*{Échantillonnage des scores avec récompenses $R_{1}$ et $R_{2}$}
Dans le cadre de l'échantillonnage avec la fonction $R_{1}$,
\begin{equation*}
  R_{1} = - \mathbb{E}[X], \quad \text{où X=1 si les blancs gagne, 0
    sinon,}
\end{equation*}
nous avons obtenons une distribution d'échantillonage fortement biaisé
en faveur des blancs. En effet, voici la distribution lissée par KDE
du score des blancs sur 1000 échantillons:


\begin{figure}[H]
  \centering
	\begin{subfigure}[b]{0.45\textwidth}
	  \centering \includegraphics[scale=0.5]{kdeblancs.png}
	  \caption{Distribution des scores \textit{centipawn} avec
            récompense $R_{1}$}
		\label{kdeblancs}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	  \centering \includegraphics[scale=0.5]{kdescoreegaux.png}
	  \caption{Distribution des scores \textit{centipawn} avec
            récompense $R_{2}$}
		\label{kdescoreegaux}
	\end{subfigure}
\end{figure}

Tel que semble l'indiquer la figure précédente, utiliser le proxy
$R_{1}$ favorise fortement les blancs, avec un score
\textit{centipawn} moyen de 2.90.  À l'inverse, en utilisant la
fonction de perte $R_{2}$, nous obtenons la distribution de score
suivante, centrée à -0.26:

Les statistiques sommaires des deux distributions sont incrites dans
la table suivante:

\begin{table}[!ht]
  \centering
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
	  \hline
	  ~     & Mean   & Std   & Min    & Max    & 25\%   & 50\%  & 75\%  \\ \hline
	  $R_1$ & 2.902  & 2.909 & -6.870 & 12.900 & 1.007  & 3.300 & 5.170 \\ \hline
	  $R_2$ & -0.267 & 1.319 & -6.17  & 4.420  & -0.502 & 0.000 & 0.390 \\ \hline
	\end{tabular}
\end{table}
Il est intéressant de noter que, quoique la moyenne soit légèrement
inférieure à 0, la médiane est \textbf{exactement} 0. Il est clair que
GFlowChess a appris à favoriser les blancs durant l'entraînement avec
la fonction $R_{1}$. Inversement, la distribution du score engendré
avec $R_{2}$ est fortement concentré autour de 0, ce qui correspond à
l'effet désiré de la fonction $R_{2}$.
\subsection*{Probabilités de victoire avec récompenses $R_{1}$ et $R_{2}$}
% TODO: À COMPLÉTER avec graphiques

\subsection*{Agressivité en fonction de $R_{1}$ et $R_{2}$}
Nous pouvons traiter l'agressivité du modèle comme étant le nombre de
pièces mangées durant une partie. Les distributions du nombre de
pièces restantes après la fin du trajectoire sont présentés dans la
figure suivante:
\begin{figure}[H]
  \centering
	\begin{subfigure}[b]{0.45\textwidth}
	  \centering \includegraphics[scale=0.5]{deadblanc.png}
	  \caption{Distribution du nombre de pièces restantes avec
            récompense $R_{1}$}
		\label{deadblancs}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	  \centering \includegraphics[scale=0.5]{deadegaux.png}
	  \caption{Distribution du nombre de pièces restantes avec
            récompense $R_{2}$}
		\label{deadegaux}
	\end{subfigure}
\end{figure}

Il est intéressant de noter la différence entre la distribution de la
figure \ref{kdeblancs} ainsi que \ref{deadblancs}.
\begin{table}[!ht]
  \centering
	\begin{tabular}{|l|l|l|l|l|l|l|l|}
	  \hline
	  ~     & Mean   & Std   & Min    & Max    & 25\%   & 50\%   & 75\%   \\ \hline
	  $R_1$ & 31.981 & 0.150 & 30.000 & 32.000 & 32.000 & 32.000 & 32.000 \\ \hline
	  $R_2$ & 31.175 & 0.925 & 27.000 & 32.000 & 31.000 & 31.000 & 32.000 \\ \hline
	\end{tabular}
\end{table}

En effet, en tenant compte du fait qu'une différence de score de 1
équivaut à un pion en moins, on peut en conclure que les blancs on
appris a éliminer des pièces de plus grande valeur, indiquant que le
modèle à appris à plus valoriser les pièces de grande valeur à l'aide
de la fonction de récompense $R_{1}$.

À l'inverse, en utilisant la fonction de récompense $R_{2}$, notre
modèle évite d'éliminer quelconque pièces. Ce comportement engendre
des parties passives.

\section*{Conclusion}
Nous avons entraîné un modèle basé sur les GFlowNets, capable de
simuler une partie d'échec à l'aide du self-play. Nous avons entraîner
le modèle avec deux fonctions de pertes différentes avec un succès
partiel. De plus, nous avons exploré les différences de comportement
engendrée par le choix de fonction de récompense. Nous avons montré
que le modèle peut apprendre la valeur intrinsèque des pièces par le
biais de la fonction de récompense et adapte son style de jeu selon la
valeur des pièces.

Plusieurs pistes de recherches restent ouvertes dans l'intersection
entre GFlowNet et les échecs. Une piste intéressante serait la
modélisation d'un environnement d'échec stochastique dans lequel
GFlowNet peut jouer contre lui-même ou bien un autre joueur. Cela
permettrait de faire apprendre au modèle à gagner des parties
d'échecs dans un environnement compétitif.


\section*{Contributions des membre de l'équipe}
\subsubsection*{Simon Théorêt}
\subsubsection*{Olivier Déry-Prévost}
\subsubsection*{Sidya Galakho}
\subsubsection*{Yizhan Li}

\bibliographystyle{latex-simple} \bibliography{references}


% supplemental material -- everything hereafter will be suppressed
%  during submission time if the hidesupplement option is provided!
\appendix


\end{document}
